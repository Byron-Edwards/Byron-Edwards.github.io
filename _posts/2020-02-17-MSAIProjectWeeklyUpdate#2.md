---
title: NTU Weekly Progress Report 20200217
author: Byron
layout: post
---

### Completion (2020/02/17 - 2020/03/08)

- Courses Assigment
  - Deep Learning with NLP final project [Proposal and presentation](https://drive.google.com/file/d/1XkSjeiGwk2YXNQhBRpScMe7P2_8cTNB3/view?usp=sharing
)(The voice is generated by the Google Text-to-Speech API)
  - Literature Review of Multi-agent system
  - Read through the book [Deep Reinforcement Learning Hands-On - Second Edition](https://learning.oreilly.com/library/view/deep-reinforcement-learning/9781838826994/)
    - Open AI Gym
    - The Cross-Entropy Method
    - Tabular Learning and the Bellman Equation
    - Deep Q-Networks
    - Higher-Level RL Libraries
    - DQN Extensions
    - Ways to Speed up RL
    - Policy Gradients – an Alternative
    - The Actor-Critic Method
    - Asynchronous Advantage Actor-Critic
  - implement the basic DQN on the Atari Game
- Other Material read:
  - [A Deep Bayesian Policy Reuse Approach Against
Non-Stationary Agen](http://papers.nips.cc/paper/7374-a-deep-bayesian-policy-reuse-approach-against-non-stationary-agents.pdf)
  - [Variational Autoencoders for Opponent Modeling in Multi-Agent Systems](https://arxiv.org/abs/2001.10829)
  - [REINFORCEMENT LEARNING (DQN) TUTORIAL](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
  - [Deep Reinforcement Learning Hands-On](https://learning.oreilly.com/library/view/deep-reinforcement-learning/9781788834247/)
  - [CoQA: A Conversational Question Answering Challenge](https://arxiv.org/abs/1808.07042)
  - [Winning Isn't Everything: Enhancing Game Development with Intelligent Agents](https://arxiv.org/abs/1903.10545)
  - [VAE Pytorch implementation](https://github.com/pytorch/examples/tree/master/vae)
  - [GAN学习指南：从原理入门到制作生成Demo](https://zhuanlan.zhihu.com/p/24767059)
  - [变分自编码器VAE：原来是这么一回事](https://zhuanlan.zhihu.com/p/34998569)
  - [花式解释AutoEncoder与VAE](https://zhuanlan.zhihu.com/p/27549418)

### Ideas

- Nonstationarity means that there is some hidden factor that influences our system dynamics, and this factor is not included in observations.
- Concrete episodes that we observe are randomly sampled from the distribution of the model, so they can differ from episode to episode. However, the probability of concrete transition to be sampled remains the same.
- When training the DQN on Atari, reduce the uncessary action the agent coud use? manuuly punish the reward for each action take?

### Questions

Basic

- Q: What the size for the batch data in Pytorch?
- A: For example, an input batch size is (3,10), the batch size is 3 and the input dim is 10
- Q: In the DQN model(acutally all RL model with randomness), it will use the random sample from the replay buff and the epsilon greedy stredgy, is that possible to completly reimplement one model later?(Throught Seed?)
- Q: For the DQN in Atari Pong, the agent tend to win the opponent using the same exactly the same way.(local optimal?)
- Q: Why it says that the max operation in the bellman equation would lead to the suboptimal policies
- Q: How to understand policy gradient
- Q: The difference between on-policy(A2C) and off-policy(DQN)
- Q: what is the typicall RL higher level library?
- Q: Any more GPU i can use?

Project related

- If one feasible method is to map the sequential decision of a opponent policy to low dimentional representation, besides the VAE, could we try other dimension reduction method? unsupervised learning?
- Q: If one paper is an imporvement based on another paper, how to make sure that the replication is accurate, specially when need to compare the performance
- Q: Why can identify the opponent policy only based on the cumulative reward during one eposide? (BRP related)
- Q: why some reference is not appeaered in the paper such as [17,19]

### Next Step

- Finish the course project for the AI Introduction (RL relatied) and Multi-agent project
